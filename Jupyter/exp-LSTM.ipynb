{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2017-10-05 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys, os, datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import functools\n",
    "#import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.model_selection as skms\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import keras.preprocessing.text\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dftrain, dftest = pd.read_csv(\"../Data/train_task1_latest.csv\"), pd.read_csv(\"../Data/test_task1_latest.csv\")\n",
    "dftrain, dftest = pd.read_csv(\"../Work/train_task1_lemma.csv\"), pd.read_csv(\"../Work/test_task1_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119398, 13),\n",
       " (74294, 12),\n",
       " Index([u'paragraph_id', u'question_id', u'paragraph', u'question', u'target',\n",
       "        u'paragraphL', u'questionL', u'paragraphLS', u'questionLS',\n",
       "        u'paragraphLX', u'questionLX', u'paragraphLSX', u'questionLSX'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.shape, dftest.shape, dftrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000) (1000, 1000) \n",
      "\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]] \n",
      "\n",
      "[[ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "x0-sum=23008.0 x1-sum=5159.0 all0=0.7340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4185,\n",
       " '\\n',\n",
       " array([ 18.,  16.,  16.,  16.,  22.,  31.,  27.,  32.,  26.,  27.,  37.,\n",
       "         21.,  16.,  41.,  22.,  21.,  23.,  24.,  18.,  30.,  21.,  37.,\n",
       "         18.,  20.,  30.,  35.,  20.,  17.,  28.,  24.,  22.,  17.,  30.,\n",
       "         15.,  24.,  16.,  19.,  32.,  26.,  24.,  20.,  29.,  20.,  27.,\n",
       "         33.,  22.,  30.,  17.,  20.,  28.,  30.,  19.,  27.,  12.,  29.,\n",
       "         23.,  35.,  16.,  28.,  13.,  14.,  20.,  25.,  26.,  33.,  20.,\n",
       "         29.,  30.,  24.,  16.,  22.,  16.,  15.,  23.,  22.,  22.,  29.,\n",
       "         27.,  29.,  22.,  20.,  16.,  16.,  22.,  12.,  24.,  22.,  17.,\n",
       "         25.,  16.,  20.,  19.,  33.,  33.,  13.,  27.,  23.,  21.,  16.,\n",
       "         16.,  28.,  20.,  17.,  28.,  40.,  27.,  18.,  17.,  30.,  19.,\n",
       "         14.,  16.,  20.,  20.,  19.,  29.,   7.,  14.,  14.,  29.,  12.,\n",
       "         12.,  29.,  19.,  22.,  18.,  16.,  12.,  25.,  12.,  35.,  30.,\n",
       "         18.,  16.,  22.,  17.,  19.,  31.,  23.,  29.,  29.,  22.,  19.,\n",
       "         23.,  22.,  38.,  24.,  22.,  16.,  17.,  17.,  31.,  33.,  28.,\n",
       "         36.,  21.,  40.,  19.,  20.,  13.,  17.,  26.,  21.,  21.,  19.,\n",
       "         24.,  35.,  18.,  16.,  14.,  20.,  17.,  27.,  29.,  31.,  15.,\n",
       "         14.,  22.,  42.,  17.,  16.,  16.,  29.,  24.,  27.,  16.,  30.,\n",
       "         19.,  22.,  24.,  21.,  37.,  19.,  25.,  21.,  20.,  18.,  17.,\n",
       "         30.,  19.,  16.,  24.,  29.,  25.,  20.,  25.,  16.,  15.,  31.,\n",
       "         22.,  24.,  20.,  20.,  48.,  23.,  31.,  17.,  24.,  14.,  20.,\n",
       "         22.,  17.,  22.,  16.,  16.,  25.,  42.,  12.,  27.,  14.,  15.,\n",
       "          7.,  23.,  20.,  17.,  14.,  22.,  16.,  26.,  24.,  22.,  25.,\n",
       "         30.,  25.,  22.,  21.,  21.,  17.,  14.,  35.,  27.,  20.,  25.,\n",
       "         27.,  33.,  35.,  15.,  27.,  24.,  20.,  30.,  17.,  20.,  13.,\n",
       "         15.,  24.,  30.,  37.,  17.,  16.,  42.,  14.,  24.,  37.,  40.,\n",
       "         40.,  23.,  17.,  23.,  19.,  26.,  16.,  23.,  18.,  28.,  15.,\n",
       "         16.,  35.,  16.,  30.,  14.,  20.,  39.,  16.,  18.,  18.,  19.,\n",
       "         21.,  17.,  26.,  35.,  24.,  16.,  27.,  17.,  17.,  31.,  24.,\n",
       "         14.,  24.,  21.,  30.,  24.,  29.,  29.,  16.,  24.,  37.,  15.,\n",
       "         15.,  20.,  22.,  12.,  17.,  16.,  29.,  17.,  37.,  20.,  20.,\n",
       "         27.,  36.,  16.,  16.,  27.,  37.,  29.,  29.,  21.,  15.,  24.,\n",
       "         25.,  20.,  21.,  29.,  17.,  25.,  19.,  37.,  16.,  27.,  15.,\n",
       "         22.,  28.,  14.,  16.,  35.,  27.,  33.,  13.,  24.,  37.,  10.,\n",
       "         26.,  40.,  14.,  12.,  16.,  17.,  19.,  27.,  16.,  44.,  17.,\n",
       "         20.,  25.,  14.,  23.,  17.,  20.,  24.,  18.,  22.,  25.,  22.,\n",
       "         22.,  22.,  22.,  27.,  10.,  16.,  21.,  27.,  37.,  27.,  23.,\n",
       "         30.,  21.,  21.,  24.,  29.,  15.,  19.,  24.,  20.,  16.,  21.,\n",
       "         19.,  22.,  18.,  37.,  27.,  24.,  17.,  17.,  13.,  29.,  16.,\n",
       "         17.,  21.,  23.,  15.,  15.,  15.,  20.,  21.,  17.,  16.,  25.,\n",
       "          6.,  12.,  31.,  16.,  22.,  37.,  30.,  27.,  17.,  16.,  19.,\n",
       "         28.,  27.,  27.,  21.,  37.,  17.,  14.,  30.,  35.,  20.,  17.,\n",
       "         16.,  23.,  24.,  41.,  16.,  15.,  18.,  24.,  35.,  16.,  25.,\n",
       "         30.,  16.,  37.,  24.,  16.,  37.,  16.,  14.,  27.,  29.,  17.,\n",
       "         20.,  21.,  32.,  24.,  24.,  20.,  29.,  30.,  31.,  26.,  14.,\n",
       "         27.,  16.,  18.,  22.,  21.,  22.,  16.,  23.,  27.,  25.,  20.,\n",
       "         15.,  30.,  24.,  25.,  17.,  35.,  42.,  52.,  27.,  31.,  23.,\n",
       "         32.,  27.,  16.,  35.,  24.,  21.,  23.,  16.,  21.,  37.,  16.,\n",
       "         28.,  30.,  24.,  13.,  24.,  19.,  25.,  24.,  11.,  29.,  19.,\n",
       "         14.,  25.,  21.,  18.,  10.,  19.,  16.,  27.,  19.,  23.,  21.,\n",
       "         25.,  29.,  15.,  16.,  18.,  24.,  24.,  24.,  24.,  37.,  29.,\n",
       "         19.,  15.,  24.,  29.,  16.,  17.,  12.,  15.,  30.,  21.,  24.,\n",
       "         16.,  30.,  18.,  23.,  17.,  18.,  22.,  40.,  16.,  16.,  30.,\n",
       "         22.,  19.,  35.,  33.,  19.,  33.,  29.,  17.,  35.,  20.,  17.,\n",
       "         24.,  17.,  21.,  20.,  16.,  20.,  26.,  20.,  35.,  37.,  29.,\n",
       "         27.,  30.,  22.,  20.,  12.,  20.,  21.,  37.,  29.,  24.,  20.,\n",
       "         15.,  21.,  26.,  26.,  14.,  19.,  20.,  16.,  16.,  18.,  50.,\n",
       "         35.,  30.,  19.,  22.,  24.,  27.,  19.,  31.,  27.,  21.,  24.,\n",
       "         27.,  20.,  17.,  15.,  35.,  16.,  20.,  29.,  35.,  21.,  24.,\n",
       "         30.,  27.,  14.,  40.,  13.,  45.,  35.,  19.,  24.,  12.,  26.,\n",
       "         22.,  21.,  19.,  30.,  17.,  30.,  26.,  37.,  28.,  53.,  20.,\n",
       "         20.,  19.,  17.,  19.,  22.,  20.,  25.,  22.,  30.,  27.,  17.,\n",
       "         11.,  14.,  24.,  14.,  27.,  26.,  30.,  29.,  16.,  16.,  29.,\n",
       "         24.,  29.,  22.,  24.,  29.,  12.,  20.,  29.,  18.,  27.,  12.,\n",
       "         29.,  19.,  35.,  18.,  29.,  12.,  32.,  23.,  15.,  37.,  29.,\n",
       "         12.,  14.,  21.,  25.,  21.,  29.,  19.,  18.,  20.,  20.,  29.,\n",
       "         15.,  32.,  26.,  29.,  30.,  24.,  31.,  21.,  16.,  20.,  17.,\n",
       "         12.,  29.,  25.,  37.,  30.,  29.,  21.,  24.,  35.,  14.,  30.,\n",
       "         30.,  15.,  26.,  21.,  19.,  29.,  14.,  20.,  30.,  38.,  26.,\n",
       "         24.,  17.,  14.,  23.,  29.,  26.,  16.,  40.,  17.,  23.,  24.,\n",
       "         16.,  17.,  17.,  27.,  26.,  23.,  24.,  27.,  38.,  20.,  14.,\n",
       "         17.,  27.,  16.,  27.,  26.,  15.,  37.,  32.,  15.,  24.,  16.,\n",
       "         24.,  16.,  16.,  18.,  16.,  26.,  21.,  17.,  17.,  23.,  15.,\n",
       "         42.,  35.,  17.,  21.,  30.,  35.,  21.,  41.,  29.,  24.,  22.,\n",
       "         24.,  28.,  26.,  26.,  21.,  22.,  16.,  16.,  27.,  21.,  16.,\n",
       "         11.,  24.,  17.,  31.,  19.,  16.,  18.,  18.,  16.,  35.,  30.,\n",
       "         20.,  20.,  35.,  25.,  29.,  27.,  14.,  35.,  11.,  27.,  17.,\n",
       "         16.,  27.,  16.,  35.,  37.,  22.,  35.,  30.,  21.,  19.,  21.,\n",
       "         23.,  27.,  30.,  24.,  25.,  23.,  22.,  34.,  25.,  21.,  26.,\n",
       "         20.,  21.,  20.,  30.,  20.,  22.,  29.,  22.,  15.,  22.,  37.,\n",
       "         17.,  31.,  33.,  16.,  21.,  17.,  11.,  16.,  23.,   8.,  25.,\n",
       "         29.,  37.,  14.,  19.,  23.,  16.,  29.,  28.,  26.,  26.,  17.,\n",
       "         22.,  17.,  31.,  23.,  16.,  14.,  13.,  20.,  45.,  31.,  29.,\n",
       "         35.,  22.,  29.,  27.,  21.,  16.,  17.,  20.,   9.,  15.,  18.,\n",
       "         22.,  21.,  17.,  17.,  14.,  15.,  24.,  16.,  17.,  20.,  19.,\n",
       "         20.,  21.,  35.,  30.,  22.,  35.,  19.,  16.,  17.,  20.,  20.,\n",
       "         22.,  29.,  32.,  21.,  17.,  35.,  16.,  16.,  20.,  46.,  13.,\n",
       "         24.,  17.,  14.,  30.,  16.,  16.,  18.,  20.,  38.,  17.,  27.,\n",
       "         29.,  20.,  16.,  17.,  24.,  22.,  26.,  15.,  18.,  17.,  14.,\n",
       "         29.,  29.,  29.,  21.,  18.,  37.,  12.,  24.,  33.,  16.,  27.,\n",
       "         21.,  25.,  29.,  23.,  21.,  31.,  25.,  20.,  21.,  17.,  35.,\n",
       "         24.,  37.,  16.,  35.,  33.,  26.,  24.,  22.,  16.,  24.]),\n",
       " array([  7.,   7.,  13.,   5.,   4.,   8.,   5.,   2.,   6.,   5.,   9.,\n",
       "          7.,   3.,   9.,  13.,   3.,   4.,   1.,  17.,   2.,   7.,   7.,\n",
       "          8.,   7.,   4.,   7.,   6.,   1.,   5.,  13.,   4.,   7.,   4.,\n",
       "         11.,   5.,  12.,   5.,   4.,   8.,   7.,   7.,   1.,   9.,   1.,\n",
       "          4.,   2.,   5.,   6.,   5.,   8.,   4.,   5.,   9.,  13.,  10.,\n",
       "          4.,   8.,   3.,   7.,   1.,   5.,   5.,   6.,   3.,   5.,   7.,\n",
       "          9.,   4.,   5.,  10.,   4.,   5.,   4.,   7.,   6.,   6.,   2.,\n",
       "          3.,  12.,   7.,   8.,   6.,   9.,   5.,   3.,   7.,   8.,   3.,\n",
       "          8.,   3.,  13.,   3.,   2.,   4.,   1.,  17.,   4.,  17.,   8.,\n",
       "          1.,  11.,   9.,   6.,   2.,   2.,  12.,   7.,   4.,   2.,   5.,\n",
       "          6.,   8.,   7.,   2.,   8.,   6.,   2.,  11.,   0.,   6.,   5.,\n",
       "          3.,   7.,   3.,   9.,   2.,  11.,   3.,   2.,   6.,  10.,   6.,\n",
       "          2.,   4.,   6.,   1.,   2.,   5.,   9.,  10.,   5.,   3.,   5.,\n",
       "          7.,   8.,   4.,   5.,   5.,   7.,   4.,   3.,   5.,   7.,   5.,\n",
       "          6.,   7.,  13.,   9.,   4.,   6.,   5.,   5.,   6.,  10.,   7.,\n",
       "          6.,   9.,   5.,   8.,   2.,   6.,  10.,   7.,   7.,   6.,   2.,\n",
       "          6.,   7.,   5.,   3.,   6.,   4.,   3.,   0.,   2.,   7.,  10.,\n",
       "          5.,   4.,   2.,   9.,   3.,   4.,   7.,   4.,   8.,   4.,  11.,\n",
       "          6.,   3.,   4.,   2.,   4.,   3.,   3.,   4.,   6.,   5.,   3.,\n",
       "          4.,   4.,   7.,   2.,   2.,   5.,   2.,   1.,   4.,   4.,   5.,\n",
       "          3.,   4.,   2.,   5.,   3.,   1.,   5.,   7.,   4.,   4.,   5.,\n",
       "          0.,   3.,   5.,   3.,   7.,   4.,   2.,   9.,   2.,   8.,   5.,\n",
       "          3.,   4.,   2.,   9.,   5.,   3.,   3.,   6.,   3.,   5.,   9.,\n",
       "          8.,  12.,   7.,   9.,   5.,   4.,   6.,   9.,   3.,   6.,   0.,\n",
       "          9.,   2.,   5.,   8.,   2.,   4.,  10.,  17.,   5.,   5.,   6.,\n",
       "         11.,   5.,   6.,   4.,   2.,   9.,   3.,   3.,   0.,   4.,   1.,\n",
       "          2.,   3.,   9.,   7.,   5.,   6.,   1.,   2.,   4.,   6.,   3.,\n",
       "          7.,  14.,   6.,   8.,   1.,   4.,   3.,   5.,  12.,   5.,   6.,\n",
       "          3.,   2.,   3.,   5.,   2.,   7.,   5.,   3.,   7.,   5.,   6.,\n",
       "          7.,   7.,   5.,   1.,   6.,  10.,   4.,   3.,   5.,   5.,   2.,\n",
       "          7.,   4.,   6.,   3.,   2.,   6.,   4.,  10.,   6.,   3.,   1.,\n",
       "          0.,   7.,   5.,   9.,   3.,   7.,   9.,   4.,   5.,   4.,   2.,\n",
       "          7.,   7.,   7.,   2.,   1.,   1.,   5.,   3.,   3.,   5.,   5.,\n",
       "          5.,  12.,  10.,   7.,   4.,   3.,   6.,   2.,   5.,  13.,   7.,\n",
       "          3.,   6.,   2.,   4.,   6.,   3.,   7.,   3.,   3.,   4.,   2.,\n",
       "          4.,   7.,   4.,   5.,   4.,   7.,   3.,   6.,   5.,   4.,  19.,\n",
       "          3.,   5.,   6.,   4.,   6.,   3.,   1.,   2.,   2.,   9.,   4.,\n",
       "          6.,   2.,   3.,   6.,   8.,   2.,  22.,   3.,   6.,  10.,   8.,\n",
       "         10.,   3.,   5.,  14.,   3.,   1.,   3.,   4.,   6.,   3.,   5.,\n",
       "          1.,   2.,   3.,   5.,  12.,   9.,   6.,   5.,   5.,   3.,   4.,\n",
       "          4.,   2.,   6.,   3.,   1.,   4.,   5.,   4.,   5.,   4.,   2.,\n",
       "          3.,   3.,  11.,   3.,   3.,   4.,   5.,   5.,   3.,   9.,   5.,\n",
       "          2.,  10.,   5.,   7.,   3.,   4.,   3.,  10.,   6.,   4.,   9.,\n",
       "          5.,   3.,   5.,   1.,   7.,   6.,   4.,   4.,   2.,   5.,   7.,\n",
       "          9.,   4.,   1.,   7.,   4.,   6.,   6.,   6.,  19.,   2.,   3.,\n",
       "          6.,   4.,   8.,   2.,   2.,   6.,   3.,   1.,   6.,   3.,   2.,\n",
       "          7.,  13.,   4.,   3.,   1.,   1.,   5.,   2.,   3.,   2.,   3.,\n",
       "          3.,   4.,   3.,   5.,   4.,   1.,   5.,   7.,   5.,   4.,   6.,\n",
       "          3.,   4.,   7.,   6.,   3.,   5.,   4.,   3.,   6.,   4.,   5.,\n",
       "         11.,   3.,   0.,   2.,   9.,   2.,   1.,   9.,   4.,   5.,   4.,\n",
       "          6.,   1.,   4.,   5.,   8.,   3.,   4.,   3.,   4.,   8.,   2.,\n",
       "          4.,   1.,   4.,   5.,   7.,   5.,   6.,   0.,   6.,   3.,   4.,\n",
       "          3.,   2.,  10.,   6.,   3.,   1.,  10.,   3.,   6.,   9.,   1.,\n",
       "          3.,   7.,   4.,   4.,   3.,   2.,   4.,  11.,   6.,   7.,   2.,\n",
       "          3.,  16.,   6.,   5.,   7.,   5.,   4.,   6.,   8.,   1.,   2.,\n",
       "          3.,   9.,   7.,   1.,   7.,   1.,   8.,   8.,   4.,   3.,   3.,\n",
       "          3.,   3.,   5.,   8.,   4.,   6.,   5.,   6.,   5.,  18.,   6.,\n",
       "          3.,   3.,   8.,   8.,   6.,   2.,   3.,   6.,   4.,   5.,   5.,\n",
       "          4.,   2.,   6.,   7.,   6.,   6.,   5.,   3.,   7.,   3.,   7.,\n",
       "          5.,   8.,   2.,   5.,   6.,  18.,   2.,   9.,   2.,   4.,   6.,\n",
       "          9.,   1.,   4.,   5.,   5.,   1.,   5.,   3.,   8.,   5.,   2.,\n",
       "          7.,   4.,   3.,   7.,   4.,   3.,   1.,   2.,   6.,   4.,   7.,\n",
       "          4.,   0.,   6.,   0.,   3.,   2.,   4.,   8.,   5.,   2.,   7.,\n",
       "         13.,   5.,  12.,   8.,   7.,   2.,   4.,   5.,   6.,   5.,   3.,\n",
       "          5.,  11.,   5.,   7.,   5.,   4.,   6.,   6.,   7.,   5.,   2.,\n",
       "          7.,   3.,   8.,   2.,   4.,   5.,  14.,   3.,   5.,   6.,   1.,\n",
       "          7.,  12.,   6.,   2.,   4.,   5.,   6.,   3.,   7.,   3.,   3.,\n",
       "          4.,   3.,   0.,   4.,   3.,  15.,   3.,  10.,   7.,   3.,  14.,\n",
       "          3.,   3.,   1.,   4.,   7.,   3.,   3.,   5.,   3.,   4.,   3.,\n",
       "          9.,   5.,   3.,   5.,   6.,   3.,   7.,   5.,  10.,   5.,   2.,\n",
       "          9.,   6.,   5.,   5.,   4.,   3.,   2.,   5.,   2.,   8.,   4.,\n",
       "          5.,   6.,   3.,   4.,  15.,   3.,   6.,   4.,   4.,   8.,   4.,\n",
       "          6.,   3.,   4.,   5.,   1.,   3.,  17.,   0.,   5.,  13.,   0.,\n",
       "          3.,   4.,   5.,   2.,   9.,   4.,   6.,   1.,   2.,   4.,   2.,\n",
       "          1.,   5.,   3.,   4.,   9.,   4.,   2.,   7.,   2.,  17.,   5.,\n",
       "          7.,   2.,   3.,   5.,   9.,   4.,   4.,  11.,   4.,   4.,  11.,\n",
       "          5.,   4.,   3.,   6.,   7.,   7.,   5.,   3.,   4.,   6.,   1.,\n",
       "          3.,  15.,   8.,   5.,   0.,   3.,   8.,   2.,   2.,   3.,   4.,\n",
       "          3.,   7.,   5.,   5.,   1.,   3.,   5.,   4.,   4.,   3.,   6.,\n",
       "          5.,   3.,   7.,   4.,   3.,   1.,   2.,   4.,   2.,   1.,   2.,\n",
       "          3.,  10.,   3.,   4.,   4.,   6.,   2.,   3.,   4.,   4.,   7.,\n",
       "          7.,   6.,   3.,  10.,   8.,   2.,   4.,   5.,   4.,   3.,   3.,\n",
       "          6.,   6.,   9.,   8.,   5.,   7.,   7.,   7.,   1.,   8.,   5.,\n",
       "          9.,   4.,   7.,   2.,   4.,   5.,   5.,   4.,  13.,   3.,   6.,\n",
       "         19.,   5.,   5.,   7.,   4.,   3.,   7.,   2.,   5.,   2.,   6.,\n",
       "          2.,  10.,   1.,   3.,   9.,  12.,   4.,   1.,   4.,   2.,   2.,\n",
       "          3.,   3.,   2.,   5.,   6.,   5.,  14.,  11.,   2.,   2.,  11.,\n",
       "          3.,  17.,   4.,   8.,   8.,   2.,   8.,   6.,   2.,   5.,   3.,\n",
       "          6.,   4.,   5.,   5.,   4.,   9.,   7.,   2.,   2.,   4.,   3.,\n",
       "          5.,   0.,   3.,   2.,   5.,   7.,   5.,   4.,   5.,   3.,   6.,\n",
       "         12.,  13.,   8.,   6.,   2.,   2.,   2.,   7.,   4.,   2.]),\n",
       " array([  0.,   0.,   2.,   0.,   3.,   1.,   1.,   2.,   6.,   0.,   9.,\n",
       "          0.,   1.,   2.,   0.,   2.,   1.,   1.,   0.,   1.,   2.,   3.,\n",
       "          6.,   5.,   1.,   1.,   0.,   0.,   4.,   0.,   2.,   6.,   0.,\n",
       "          1.,   5.,   0.,   4.,   4.,   2.,   6.,   7.,   0.,   1.,   0.,\n",
       "          4.,   0.,   5.,   1.,   1.,   8.,   1.,   3.,   2.,   2.,   2.,\n",
       "          4.,   0.,   1.,   2.,   0.,   0.,   1.,   4.,   1.,   0.,   0.,\n",
       "          2.,   2.,   1.,   1.,   3.,   1.,   4.,   1.,   4.,   1.,   2.,\n",
       "          0.,   2.,   0.,   2.,   2.,   0.,   4.,   1.,   1.,   0.,   2.,\n",
       "          4.,   1.,  11.,   0.,   1.,   3.,   0.,   0.,   1.,   0.,   0.,\n",
       "          0.,   9.,   0.,   0.,   2.,   2.,   1.,   1.,   1.,   0.,   1.,\n",
       "          1.,   0.,   0.,   0.,   2.,   1.,   1.,   0.,   0.,   1.,   0.,\n",
       "          2.,   0.,   3.,   5.,   0.,   1.,   0.,   2.,   0.,   3.,   1.,\n",
       "          0.,   1.,   1.,   1.,   1.,   1.,   9.,   1.,   1.,   0.,   5.,\n",
       "          2.,   3.,   4.,   1.,   4.,   0.,   1.,   2.,   1.,   3.,   1.,\n",
       "          5.,   0.,   0.,   0.,   1.,   5.,   2.,   5.,   6.,   2.,   1.,\n",
       "          2.,   1.,   0.,   0.,   1.,   4.,   0.,   2.,   0.,   0.,   2.,\n",
       "          2.,   7.,   3.,   2.,   0.,   1.,   3.,   0.,   2.,   1.,   0.,\n",
       "          1.,   0.,   0.,   0.,   2.,   3.,   4.,   0.,   0.,   1.,   0.,\n",
       "          0.,   1.,   4.,   1.,   0.,   2.,   3.,   2.,   0.,   0.,   0.,\n",
       "          0.,   1.,   2.,   2.,   2.,   4.,   0.,   1.,   0.,   0.,   4.,\n",
       "          2.,   4.,   2.,   4.,   0.,   1.,   3.,   0.,   3.,   0.,   4.,\n",
       "          0.,   3.,   2.,   1.,   1.,   0.,   0.,   2.,   0.,   0.,   5.,\n",
       "          2.,   2.,   1.,   1.,   2.,   0.,   3.,   1.,   1.,   1.,   9.,\n",
       "          2.,   0.,   1.,   1.,   2.,   1.,   0.,   3.,   0.,   0.,   0.,\n",
       "          0.,   2.,   2.,   0.,   1.,   1.,  10.,   0.,   1.,   1.,   0.,\n",
       "          4.,   2.,   0.,   4.,   1.,   1.,   3.,   1.,   0.,   1.,   1.,\n",
       "          1.,   0.,   1.,   1.,   0.,   2.,   1.,   1.,   0.,   0.,   2.,\n",
       "          1.,   2.,   2.,   2.,   1.,   0.,   0.,   0.,   0.,   1.,   2.,\n",
       "          0.,   1.,   3.,   2.,   1.,   1.,   1.,   1.,   0.,   1.,   0.,\n",
       "          1.,   1.,   4.,   1.,   0.,   0.,   0.,   1.,   5.,   4.,   2.,\n",
       "          1.,   4.,   0.,   1.,   2.,   2.,   3.,   3.,   1.,   1.,   1.,\n",
       "          0.,   7.,   0.,   1.,   1.,   2.,   9.,   2.,   0.,   0.,   2.,\n",
       "          3.,   6.,   2.,   2.,   1.,   1.,   5.,   1.,   1.,   5.,   0.,\n",
       "          1.,   1.,   1.,   0.,   1.,   2.,   1.,   2.,   1.,  13.,   0.,\n",
       "          2.,   0.,   1.,   2.,   1.,   2.,   1.,   3.,   0.,   0.,   1.,\n",
       "          3.,   0.,   4.,   4.,   2.,   1.,   1.,   0.,   0.,   2.,   1.,\n",
       "          1.,   0.,   2.,   1.,   1.,   1.,   1.,   1.,   2.,   0.,   1.,\n",
       "          0.,   2.,   1.,   6.,   3.,   1.,   1.,   0.,   6.,   2.,   0.,\n",
       "          0.,   3.,   0.,   1.,   3.,   0.,   0.,   2.,   3.,   1.,   2.,\n",
       "          0.,   2.,   1.,   1.,   1.,   2.,   0.,   2.,   1.,   1.,   2.,\n",
       "          2.,   1.,   2.,   1.,   1.,   0.,   0.,   2.,   1.,   4.,   2.,\n",
       "          1.,   2.,   2.,   2.,   3.,   0.,   1.,   1.,   1.,   0.,   0.,\n",
       "          1.,   1.,   1.,   0.,   1.,   3.,   3.,   1.,   1.,   1.,   0.,\n",
       "          2.,   1.,   5.,   0.,   1.,   1.,   3.,   1.,   0.,   5.,   1.,\n",
       "          0.,   0.,   1.,   0.,   4.,   0.,   2.,   4.,   1.,   2.,   1.,\n",
       "          0.,   1.,   1.,   2.,   2.,   1.,   3.,   1.,   0.,   3.,   2.,\n",
       "          7.,   1.,   0.,   1.,   1.,   1.,   4.,   1.,   1.,   0.,   2.,\n",
       "          0.,   3.,   1.,   2.,   1.,   1.,   3.,   2.,   5.,   4.,   2.,\n",
       "          1.,   0.,   3.,   3.,   1.,   0.,   1.,   3.,   0.,   4.,   1.,\n",
       "          1.,   1.,   0.,   1.,   0.,   0.,   1.,   1.,   2.,   1.,   4.,\n",
       "          0.,   0.,   0.,   1.,   0.,   1.,   2.,   2.,   1.,   2.,   1.,\n",
       "          1.,   1.,   0.,   0.,   0.,   0.,   6.,   0.,   0.,   1.,   2.,\n",
       "          3.,   2.,   2.,   0.,   1.,   1.,   0.,   2.,   1.,   0.,   1.,\n",
       "          2.,   1.,   1.,   0.,   2.,   1.,   2.,   4.,   1.,   3.,   2.,\n",
       "          0.,   0.,   0.,   0.,   2.,   2.,   3.,   3.,   0.,   0.,   0.,\n",
       "          3.,   2.,   1.,   1.,   0.,   1.,   2.,   8.,   0.,   0.,   1.,\n",
       "          0.,   0.,   1.,   0.,   2.,   0.,   1.,   1.,   0.,   0.,   1.,\n",
       "          0.,   0.,   0.,   0.,   1.,   0.,   1.,   0.,   0.,   1.,   0.,\n",
       "          1.,   2.,   0.,   1.,   4.,   2.,   5.,   3.,   2.,   0.,   3.,\n",
       "          2.,   8.,   2.,   2.,   0.,   1.,   2.,   1.,   2.,   4.,   1.,\n",
       "          2.,   1.,   4.,   3.,   1.,   1.,   1.,   1.,   1.,   0.,   2.,\n",
       "          6.,   1.,   2.,   0.,   0.,   1.,   0.,   2.,   0.,   2.,   1.,\n",
       "          1.,   0.,   0.,   0.,   0.,   1.,   3.,   6.,   1.,   2.,   7.,\n",
       "          1.,   0.,   1.,   0.,   0.,   0.,   4.,   0.,   1.,   0.,   0.,\n",
       "          3.,   0.,   0.,   6.,   2.,   0.,   1.,   3.,   0.,   0.,   1.,\n",
       "          0.,   0.,   1.,   1.,   0.,   2.,   1.,   2.,   0.,   2.,   1.,\n",
       "          7.,   0.,   1.,   0.,   1.,   2.,   0.,   1.,   1.,   0.,   1.,\n",
       "          4.,   3.,   0.,   4.,   2.,   1.,   1.,   2.,   1.,   2.,   1.,\n",
       "          1.,   2.,   0.,   0.,   1.,   1.,   1.,   1.,   3.,   0.,   1.,\n",
       "          0.,   2.,   0.,   4.,   1.,   1.,   1.,   1.,   2.,   0.,   2.,\n",
       "          0.,   0.,   1.,   0.,   2.,   0.,   1.,   4.,   2.,   0.,   1.,\n",
       "          1.,   1.,   2.,   4.,   0.,   3.,   1.,   1.,   1.,   2.,   1.,\n",
       "          2.,   1.,   2.,   1.,   0.,   2.,   0.,   0.,   0.,   2.,   0.,\n",
       "          3.,   2.,   2.,   1.,   0.,   4.,   0.,   0.,   1.,   0.,   2.,\n",
       "          1.,   0.,   1.,   0.,   1.,   1.,   2.,   7.,   1.,   0.,   2.,\n",
       "          0.,   1.,   3.,   2.,   0.,   0.,   2.,   0.,   3.,   0.,   1.,\n",
       "          0.,   0.,   1.,   1.,   3.,   0.,   1.,   2.,   1.,   1.,   1.,\n",
       "          2.,   1.,   3.,   4.,   0.,   1.,   0.,   1.,   0.,   1.,   2.,\n",
       "          1.,   5.,   1.,   2.,   1.,   0.,   0.,   3.,   4.,   0.,   0.,\n",
       "          1.,   3.,   3.,   1.,   2.,   0.,   2.,   0.,   2.,   1.,   1.,\n",
       "          0.,   2.,   1.,   3.,   4.,   0.,   2.,   3.,   4.,   1.,   2.,\n",
       "          7.,   5.,   0.,   0.,   1.,   0.,   1.,   2.,   1.,   0.,   2.,\n",
       "          0.,   0.,   0.,   2.,   1.,   1.,   3.,   1.,   1.,   1.,   1.,\n",
       "          0.,   1.,   1.,   1.,   3.,   0.,   1.,   0.,   1.,   1.,   3.,\n",
       "          1.,   3.,   0.,   2.,   4.,   3.,   1.,   1.,   0.,   1.,   1.,\n",
       "          0.,   1.,   0.,   1.,   3.,   0.,   1.,   0.,   1.,   2.,   1.,\n",
       "          0.,   0.,   2.,   1.,   0.,   1.,   1.,   2.,   2.,   0.,   1.,\n",
       "          0.,   2.,   0.,   3.,   1.,   0.,   7.,   2.,   2.,   2.,   2.,\n",
       "          1.,   1.,   0.,   1.,   0.,   0.,   2.,   2.,   2.,   1.,   1.,\n",
       "          1.,   0.,   2.,   2.,   5.,   1.,   1.,   0.,   0.,   1.,   1.,\n",
       "          0.,  12.,   0.,   0.,   2.,   1.,   1.,   0.,   2.,   0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "         1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows  = 1000 #len(dftrain) #10000 #00\n",
    "train_x0  = dftrain.paragraphLS[:num_rows].tolist()\n",
    "train_x1  = dftrain.questionLS[:num_rows].tolist()\n",
    "xxx = []\n",
    "for xx in train_x1 :\n",
    "    xxx.append(' '.join([ss.encode('utf-8') for ss in re.findall(\"\\w+\", xx.decode('utf-8'),re.UNICODE) if len(ss)>=3]))\n",
    "#print(train_x1[:2])\n",
    "#print(xxx[:2])\n",
    "#print([xx for xx in xxx if xx in None])\n",
    "train_x1 = xxx\n",
    "train_y   = dftrain.target[:num_rows]\n",
    "train_voc = train_x1 #+train_x0\n",
    "tokens = keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "xx = tokens.fit_on_texts(train_voc)\n",
    "\n",
    "train_x0 = tokens.texts_to_matrix(train_x0, mode='binary')  #, mode='tfidf')\n",
    "train_x1 = tokens.texts_to_matrix(train_x1, mode='binary')  #, mode='tfidf')\n",
    "'''\n",
    "train_x0 = tokens.sequences_to_matrix(train_x0, mode='binary')  #, mode='tfidf')\n",
    "train_x1 = tokens.sequences_to_matrix(train_x1, mode='binary')  #, mode='tfidf')\n",
    "'''\n",
    "print train_x0.shape, train_x1.shape, '\\n\\n', train_x0[:4,:6], '\\n\\n', train_x1[:4,:6]\n",
    "print  '\\nx0-sum={} x1-sum={} all0={:.4f}'.format(train_x0.sum(),train_x1.sum(), 1.0-float(train_y.sum())/len(train_y))\n",
    "\n",
    "train_yy = np.array([train_y.values,train_y.values]).T\n",
    "train_yy[:,0] = 1.0-train_yy[:,1]\n",
    "\n",
    "train_yy.shape, train_yy[:100,0]\n",
    "\n",
    "\n",
    "#tokens.__dict__\n",
    "len(tokens.word_counts), '\\n', train_x0.sum(axis=1), train_x1.sum(axis=1), (train_x0*train_x1).sum(axis=1), train_y.values\n",
    "#for tt,xx in zip(tokens.word_counts.keys()[:55],tokens.word_counts.values()[:55]) : print xx,tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUB0NwmzddkEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39249f9b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3924c7d7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_x1.sum()\n",
    "plt.hist(train_x1.flatten()); plt.show()\n",
    "plt.hist(train_y.values.flatten()); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 16)      16000       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 8)       8000        input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 128)           74240       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 128)           70144       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 128)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 16)            2064        add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            1088        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             130         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 171,666\n",
      "Trainable params: 171,666\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.layers.merge\n",
    "max_features = train_x0.shape[1]\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(max_features, 256))\n",
    "#model.add(Input(shape=[max_feature], dtype='float32'))\n",
    "#model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,input_shape=[None,6]))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "i0 = Input(shape=[max_features]) #, dtype='float32')\n",
    "x0 = Embedding(max_features, 16)(i0)\n",
    "x0 = LSTM(128,dropout=0.2)(x0)\n",
    "\n",
    "i1 = Input(shape=[max_features]) #, dtype='float32')\n",
    "x1 = Embedding(max_features, 8)(i1)\n",
    "x1 = LSTM(128,dropout=0.2)(x1)\n",
    "\n",
    "#xx = keras.layers.concatenate([x0, x1])\n",
    "xx = keras.layers.Add()([x0, x1])\n",
    "\n",
    "xx = Dense(16, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "xx = Dense(64, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "#xx = Dense(128, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "##xx = Dense(256, activation='relu')(xx)\n",
    "xx = Dropout(0.25)(xx)\n",
    "o = Dense(2, activation='softmax')(xx)\n",
    "#o = Dense(1, activation='sigmoid')(xx)\n",
    "\n",
    "model = Model(inputs=[i0,i1], outputs=o)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', #'mae', #'mse', #'binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy','mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 16)      16000       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 16)      16000       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 16), (None, 1 2112        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, 16), (None, 1 2112        embedding_2[0][0]                \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           2176        lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             258         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 38,658\n",
      "Trainable params: 38,658\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = train_x0.shape[1]\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(max_features, 256))\n",
    "#model.add(Input(shape=[max_feature], dtype='float32'))\n",
    "#model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,input_shape=[None,6]))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "i0 = Input(shape=[max_features]) #, dtype='float32')\n",
    "x0 = Embedding(max_features,16)(i0)\n",
    "x0,state_h,state_c = LSTM(16,dropout=0.2,return_state=True)(x0)\n",
    "\n",
    "i1 = Input(shape=[max_features]) #, dtype='float32')\n",
    "x1 = Embedding(max_features, 16)(i1)\n",
    "xx,_,_ = LSTM(16,dropout=0.2,return_state=True)(x1,initial_state=[state_h,state_c])\n",
    "\n",
    "#xx = keras.layers.concatenate([x0, x1])\n",
    "\n",
    "#xx = Dense(128, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "#xx = Dense(64, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "#xx = Dense(32, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "#xx = Dense(16, activation='relu')(xx)\n",
    "#xx = Dropout(0.25)(xx)\n",
    "xx = Dense(128, activation='relu')(xx)\n",
    "xx = Dropout(0.25)(xx)\n",
    "##xx = Dense(256, activation='relu')(xx)\n",
    "###xx = Dropout(0.25)(xx)\n",
    "o = Dense(2, activation='softmax')(xx)\n",
    "#o = Dense(1, activation='sigmoid')(xx)\n",
    "\n",
    "model = Model(inputs=[i0,i1], outputs=o)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=0.3)\n",
    "model.compile(loss='binary_crossentropy', #'mae', #'mse', #'binary_crossentropy',\n",
    "              optimizer='RMSprop', #sgd, #'adam',\n",
    "              metrics=['binary_accuracy','accuracy','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-07 21:24:11.951716\n",
      "Epoch 1/10\n",
      "16s - loss: 0.5965 - binary_accuracy: 0.7340 - mean_squared_error: 0.2023\n",
      "Epoch 2/10\n",
      "16s - loss: 0.5825 - binary_accuracy: 0.7340 - mean_squared_error: 0.1967\n",
      "Epoch 3/10\n",
      "16s - loss: 0.5834 - binary_accuracy: 0.7340 - mean_squared_error: 0.1969\n",
      "Epoch 4/10\n",
      "16s - loss: 0.5805 - binary_accuracy: 0.7340 - mean_squared_error: 0.1958\n",
      "Epoch 5/10\n",
      "16s - loss: 0.5826 - binary_accuracy: 0.7340 - mean_squared_error: 0.1966\n",
      "Epoch 6/10\n",
      "16s - loss: 0.5850 - binary_accuracy: 0.7340 - mean_squared_error: 0.1976\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000500000023749.\n",
      "16s - loss: 0.5808 - binary_accuracy: 0.7340 - mean_squared_error: 0.1958\n",
      "Epoch 8/10\n",
      "16s - loss: 0.5804 - binary_accuracy: 0.7340 - mean_squared_error: 0.1957\n",
      "Epoch 9/10\n",
      "16s - loss: 0.5839 - binary_accuracy: 0.7340 - mean_squared_error: 0.1971\n",
      "Epoch 10/10\n",
      "16s - loss: 0.5819 - binary_accuracy: 0.7340 - mean_squared_error: 0.1963\n",
      "2017-10-07 21:26:54.833149\n"
     ]
    }
   ],
   "source": [
    "batch_size   = 128  #128 #256 #512 #1024 #+512 #+1024 #+512\n",
    "print (datetime.datetime.now())\n",
    "\n",
    "reduce_lr1     = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.0000001,verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10,min_delta=0.0001,verbose=1)\n",
    "\n",
    "#filepath=\"../Temp/TempTempG5M5KFold2/V1-UNET-val_\"+metric+\"={val_\"+metric+\":.4f}-{\"+metric+\":.4f}---\"+str(i)+\".hdf5\"\n",
    "#checkpoint1 = ModelCheckpoint(filepath, monitor=metric,        verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "hist = model.fit([train_x0,train_x1], train_yy,\n",
    "                  batch_size=batch_size,\n",
    "                  #shuffle=True,\n",
    "                  epochs=10,\n",
    "                  initial_epoch=0,\n",
    "                  callbacks=[reduce_lr1],\n",
    "                  #validation_data=([x_train[-200:],z_train[-200:],a_train[-200:]], y_train[-200:]),\n",
    "                  verbose=2)\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hist.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_xp = model.predict([train_x0[:num_rows],train_x1[:num_rows]],batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.69999999999999996, 0.5)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skm.accuracy_score(train_yy[:num_rows],train_xp>0.5), skm.roc_auc_score(train_yy[:num_rows],train_xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37e06b7350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_xp[:,1].sum()\n",
    "hh=plt.hist(train_xp[:,1].flatten(),bins=50); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  30.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]),\n",
       " array([ -1.98198259e-02,   1.80174112e-04,   2.01801741e-02,\n",
       "          4.01801741e-02,   6.01801741e-02,   8.01801741e-02,\n",
       "          1.00180174e-01,   1.20180174e-01,   1.40180174e-01,\n",
       "          1.60180174e-01,   1.80180174e-01,   2.00180174e-01,\n",
       "          2.20180174e-01,   2.40180174e-01,   2.60180174e-01,\n",
       "          2.80180174e-01,   3.00180174e-01,   3.20180174e-01,\n",
       "          3.40180174e-01,   3.60180174e-01,   3.80180174e-01,\n",
       "          4.00180174e-01,   4.20180174e-01,   4.40180174e-01,\n",
       "          4.60180174e-01,   4.80180174e-01,   5.00180174e-01,\n",
       "          5.20180174e-01,   5.40180174e-01,   5.60180174e-01,\n",
       "          5.80180174e-01,   6.00180174e-01,   6.20180174e-01,\n",
       "          6.40180174e-01,   6.60180174e-01,   6.80180174e-01,\n",
       "          7.00180174e-01,   7.20180174e-01,   7.40180174e-01,\n",
       "          7.60180174e-01,   7.80180174e-01,   8.00180174e-01,\n",
       "          8.20180174e-01,   8.40180174e-01,   8.60180174e-01,\n",
       "          8.80180174e-01,   9.00180174e-01,   9.20180174e-01,\n",
       "          9.40180174e-01,   9.60180174e-01,   9.80180174e-01]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269,\n",
       " '\\n',\n",
       " array([  7.,   8.,   7.,   4.,  19.,   9.,  10.,  16.,  17.,  10.,  35.,\n",
       "         10.,  12.,  14.,   6.,  22.,   4.,  14.,   6.,  15.,   5.,  32.,\n",
       "         23.,  17.,  12.,  12.,  14.,  13.,  25.,  17.]),\n",
       " array([ 12.,  18.,  16.,   8.,   5.,  12.,   8.,   8.,  10.,   7.,  14.,\n",
       "         11.,   7.,  14.,  13.,   6.,   6.,   5.,  17.,   5.,   8.,  10.,\n",
       "         15.,   9.,   7.,   9.,   9.,   3.,   8.,  14.]),\n",
       " array([  0.,   0.,   2.,   0.,   3.,   1.,   2.,   7.,  11.,   0.,  31.,\n",
       "          0.,   8.,   3.,   0.,  12.,   1.,   4.,   0.,   1.,   2.,  13.,\n",
       "         23.,   7.,   3.,   1.,   0.,   4.,  11.,   0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.]),\n",
       " array([ 0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017,\n",
       "         0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017,\n",
       "         0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017,\n",
       "         0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017,\n",
       "         0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017,\n",
       "         0.48018017,  0.48018017,  0.48018017,  0.48018017,  0.48018017], dtype=float32))"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens.word_counts), '\\n', train_x0.sum(axis=1), train_x1.sum(axis=1), (train_x0*train_x1).sum(axis=1), train_y.values, train_xp[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "xx = dftrain.paragraphLS[:num_rows].tolist()\n",
    "#print(type(xx),unicode(xx[0],'utf-8'))\n",
    "\n",
    "##ff=nltk.probability.FreqDist(xx[0].decode('utf-8'))\n",
    "##ff=nltk.probability.FreqDist(unicode(xx[0],'utf-8'),unicode_)\n",
    "\n",
    "ff=nltk.probability.FreqDist()\n",
    "\n",
    "for ss in xx :\n",
    "    for ww in re.findall(\"\\w+\", ss.decode('utf-8'),re.UNICODE) :\n",
    "        ff[ww] +=1\n",
    "\n",
    "\n",
    "#for ww,cc in ff.most_common(50) : print(ww,cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 134808\n",
      " 84027\n",
      " 59027\n",
      " 55298\n",
      " 46071\n",
      " 40857\n",
      " 33663\n",
      " 33155\n",
      " 28849\n",
      " 27409\n",
      " 25191\n",
      " 25005\n",
      " 23112\n",
      " 22024\n",
      " 21269\n",
      " 19141\n",
      " 19004\n",
      " 18984\n",
      " 18482\n",
      " 18071\n",
      " 17594\n",
      " 17112\n",
      " 17066\n",
      " 16918\n",
      " 16481\n",
      " 16471\n",
      " 16230\n",
      " 16189\n",
      " 15804\n",
      " 14612\n",
      " 14559\n",
      " 14396\n",
      " 14138\n",
      " 13909\n",
      " 13904\n",
      " 13754\n",
      " 13682\n",
      " 13438\n",
      " 13414\n",
      " 13341\n",
      "4 13260\n",
      "1 13211\n",
      " 13026\n",
      " 12871\n",
      " 12618\n",
      " 12602\n",
      " 12460\n",
      " 12431\n",
      " 12080\n",
      " 11915\n"
     ]
    }
   ],
   "source": [
    "for ww,cc in ff.most_common(50) :\n",
    "    print ww,cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
